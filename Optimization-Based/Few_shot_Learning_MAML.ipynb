{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPj2J07s/cN6ltSHUlj9JHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilesh-K-Yadav/Meta-Learning_Projects/blob/main/Optimization-Based/Few_shot_Learning_MAML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HfKZDX81uDyV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import google_drive_downloader as gdd\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import dataset, sampler, dataloader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "NUM_TRAIN_CLASSES = 1100\n",
        "NUM_VAL_CLASSES = 100\n",
        "NUM_TEST_CLASSES = 423\n",
        "NUM_SAMPLES_PER_CLASS = 20\n",
        "\n",
        "NUM_INPUT_CHANNELS = 1\n",
        "NUM_HIDDEN_CHANNELS = 64\n",
        "KERNEL_SIZE = 3\n",
        "NUM_CONV_LAYERS = 4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "NUM_TEST_TASKS = 600\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = './omniglot_resized'\n",
        "GDD_FILE_ID = '1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI'\n",
        "gdd.GoogleDriveDownloader.download_file_from_google_drive(\n",
        "                file_id=GDD_FILE_ID,\n",
        "                dest_path=f'{BASE_PATH}.zip',\n",
        "                unzip=True\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wpmJYdzuomL",
        "outputId": "38415b86-b8f0-4443-e599-5d42b528eabc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI into ./omniglot_resized.zip... Done.\n",
            "Unzipping...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(file_path):\n",
        "    \"\"\"Loads and transforms an Omniglot image.\n",
        "    Args:\n",
        "        file_path (str): file path of image\n",
        "    Returns:\n",
        "        a Tensor containing image data\n",
        "            shape (1, 28, 28)\n",
        "    \"\"\"\n",
        "    x = imageio.imread(file_path)\n",
        "    x = torch.tensor(x, dtype=torch.float32).reshape([1, 28, 28])\n",
        "    x = x / 255.0\n",
        "    return 1 - x"
      ],
      "metadata": {
        "id": "X6o-P5CdvMsp"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotDataset(dataset.Dataset):\n",
        "    \"\"\"Omniglot dataset for meta-learning.\n",
        "\n",
        "    Each element of the dataset is a task. A task is specified with a key,\n",
        "    which is a tuple of class indices (no particular order). The corresponding\n",
        "    value is the instantiated task, which consists of sampled (image, label)\n",
        "    pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_support, num_query):\n",
        "        \"\"\"Inits OmniglotDataset.\n",
        "\n",
        "        Args:\n",
        "            num_support (int): number of support examples per class\n",
        "            num_query (int): number of query examples per class\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # get all character folders\n",
        "        self._character_folders = glob.glob(\n",
        "            os.path.join(BASE_PATH, '*/*/'))\n",
        "        assert len(self._character_folders) == (\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
        "        )\n",
        "\n",
        "        # shuffle characters\n",
        "        np.random.default_rng(0).shuffle(self._character_folders)\n",
        "\n",
        "        # check problem arguments\n",
        "        assert num_support + num_query <= NUM_SAMPLES_PER_CLASS\n",
        "        self._num_support = num_support\n",
        "        self._num_query = num_query\n",
        "\n",
        "    def __getitem__(self, class_idxs):\n",
        "        \"\"\"Constructs a task.\n",
        "\n",
        "        Data for each class is sampled uniformly at random without replacement.\n",
        "        The ordering of the labels corresponds to that of class_idxs.\n",
        "\n",
        "        Args:\n",
        "            class_idxs (tuple[int]): class indices that comprise the task\n",
        "\n",
        "        Returns:\n",
        "            images_support (Tensor): task support images\n",
        "                shape (num_way * num_support, channels, height, width)\n",
        "            labels_support (Tensor): task support labels\n",
        "                shape (num_way * num_support,)\n",
        "            images_query (Tensor): task query images\n",
        "                shape (num_way * num_query, channels, height, width)\n",
        "            labels_query (Tensor): task query labels\n",
        "                shape (num_way * num_query,)\n",
        "        \"\"\"\n",
        "        images_support, images_query = [], []\n",
        "        labels_support, labels_query = [], []\n",
        "\n",
        "        for label, class_idx in enumerate(class_idxs):\n",
        "            # get a class's examples and sample from them\n",
        "            all_file_paths = glob.glob(\n",
        "                os.path.join(self._character_folders[class_idx], '*.png')\n",
        "            )\n",
        "            sampled_file_paths = np.random.default_rng().choice(\n",
        "                all_file_paths,\n",
        "                size=self._num_support + self._num_query,\n",
        "                replace=False\n",
        "            )\n",
        "            images = [load_image(file_path) for file_path in sampled_file_paths]\n",
        "\n",
        "            # split sampled examples into support and query\n",
        "            images_support.extend(images[:self._num_support])\n",
        "            images_query.extend(images[self._num_support:])\n",
        "            labels_support.extend([label] * self._num_support)\n",
        "            labels_query.extend([label] * self._num_query)\n",
        "\n",
        "        # aggregate into tensors\n",
        "        images_support = torch.stack(images_support)  # shape (N*S, C, H, W)\n",
        "        labels_support = torch.tensor(labels_support)  # shape (N*S)\n",
        "        images_query = torch.stack(images_query)\n",
        "        labels_query = torch.tensor(labels_query)\n",
        "\n",
        "        return images_support, labels_support, images_query, labels_query\n"
      ],
      "metadata": {
        "id": "a8dqXXMiGEqV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotSampler(sampler.Sampler):\n",
        "    \"\"\"Samples task specification keys for an OmniglotDataset.\"\"\"\n",
        "\n",
        "    def __init__(self, split_idxs, num_way, num_tasks):\n",
        "\n",
        "      \"\"\"Inits OmniglotSampler.\n",
        "\n",
        "      Args:\n",
        "          split_idxs (range): indices that comprise the\n",
        "              training/validation/test split\n",
        "          num_way (int): number of classes per task\n",
        "          num_tasks (int): number of tasks to sample\n",
        "      \"\"\"\n",
        "      super().__init__(None)\n",
        "\n",
        "      self.split_idxs = split_idxs\n",
        "      self.num_way = num_way\n",
        "      self.num_tasks = num_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "      return (\n",
        "          np.random.default_rng().choice(\n",
        "              self.split_idxs,\n",
        "              size=self.num_way,\n",
        "              replace=False\n",
        "          ) for _ in range(self.num_tasks)\n",
        "      )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_tasks"
      ],
      "metadata": {
        "id": "ogefBquxdV0g"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def get_omniglot_dataloader(\n",
        "        split,\n",
        "        batch_size,\n",
        "        num_way,\n",
        "        num_support,\n",
        "        num_query,\n",
        "        num_tasks_per_epoch\n",
        "):\n",
        "    \"\"\"Returns a dataloader.DataLoader for Omniglot.\n",
        "\n",
        "    Args:\n",
        "        split (str): one of 'train', 'val', 'test'\n",
        "        batch_size (int): number of tasks per batch\n",
        "        num_way (int): number of classes per task\n",
        "        num_support (int): number of support examples per class\n",
        "        num_query (int): number of query examples per class\n",
        "        num_tasks_per_epoch (int): number of tasks before DataLoader is\n",
        "            exhausted\n",
        "    \"\"\"\n",
        "\n",
        "    if split == 'train':\n",
        "        split_idxs = range(NUM_TRAIN_CLASSES)\n",
        "    elif split == 'val':\n",
        "        split_idxs = range(\n",
        "            NUM_TRAIN_CLASSES,\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES\n",
        "        )\n",
        "    elif split == 'test':\n",
        "        split_idxs = range(\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES,\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    return dataloader.DataLoader(\n",
        "        dataset=OmniglotDataset(num_support, num_query),\n",
        "        batch_size=batch_size,\n",
        "        sampler=OmniglotSampler(split_idxs, num_way, num_tasks_per_epoch),\n",
        "        num_workers=2,\n",
        "        collate_fn=identity,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True\n",
        "    )"
      ],
      "metadata": {
        "id": "PbvVAMUy-4hY"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = imageio.imread(\"/content/omniglot_resized/Alphabet_of_the_Magi/character01/0709_01.png\")\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqXJUyu1HRWE",
        "outputId": "68c18709-2292-40b1-a072-ef2800141e63"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-913cf8e1afe4>:1: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(\"/content/omniglot_resized/Alphabet_of_the_Magi/character01/0709_01.png\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = get_omniglot_dataloader(\n",
        "        split='train',\n",
        "        batch_size=1,\n",
        "        num_way=4,\n",
        "        num_support=1,\n",
        "        num_query=1,\n",
        "        num_tasks_per_epoch=1\n",
        ")\n",
        "tasks = next(iter(samples))\n",
        "for task in tasks:\n",
        "  images_support, labels_support, images_query, labels_query = task\n",
        "images_support.shape, labels_support.shape, images_query.shape, labels_query.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwOIUtCOHmOJ",
        "outputId": "90abb6c8-8545-419d-c96a-46b9024824be"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-a6327f0e4a5e>:9: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  x = imageio.imread(file_path)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 1, 28, 28]),\n",
              " torch.Size([4]),\n",
              " torch.Size([4, 1, 28, 28]),\n",
              " torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images_support[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "zytM404OI-kT",
        "outputId": "6f8375d9-623a-4fc5-e237-40379a5d9e54"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7cd628048eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWZ8ZeLtMjlL",
        "outputId": "b1f1a1e8-a038-477c-a54b-b52c86de41f3"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score(logits, labels):\n",
        "    \"\"\"Returns the mean accuracy of a model's predictions on a set of examples.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): model predicted logits\n",
        "            shape (examples, classes)\n",
        "        labels (torch.Tensor): classification labels from 0 to num_classes - 1\n",
        "            shape (examples,)\n",
        "    \"\"\"\n",
        "\n",
        "    assert logits.dim() == 2\n",
        "    assert labels.dim() == 1\n",
        "    assert logits.shape[0] == labels.shape[0]\n",
        "    y = torch.argmax(logits, dim=-1) == labels\n",
        "    y = y.type(torch.float)\n",
        "    return torch.mean(y).item()"
      ],
      "metadata": {
        "id": "uoQAD2PyOml3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MAML:\n",
        "    \"\"\"Trains and assesses a MAML.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_outputs,\n",
        "            num_inner_steps,\n",
        "            inner_lr,\n",
        "            learn_inner_lrs,\n",
        "            outer_lr,\n",
        "            log_dir\n",
        "    ):\n",
        "        \"\"\"Inits MAML.\n",
        "\n",
        "        The network consists of four convolutional blocks followed by a linear\n",
        "        head layer. Each convolutional block comprises a convolution layer, a\n",
        "        batch normalization layer, and ReLU activation.\n",
        "\n",
        "        Note that unlike conventional use, batch normalization is always done\n",
        "        with batch statistics, regardless of whether we are training or\n",
        "        evaluating. This technically makes meta-learning transductive, as\n",
        "        opposed to inductive.\n",
        "\n",
        "        Args:\n",
        "            num_outputs (int): dimensionality of output, i.e. number of classes\n",
        "                in a task\n",
        "            num_inner_steps (int): number of inner-loop optimization steps\n",
        "            inner_lr (float): learning rate for inner-loop optimization\n",
        "                If learn_inner_lrs=True, inner_lr serves as the initialization\n",
        "                of the learning rates.\n",
        "            learn_inner_lrs (bool): whether to learn the above\n",
        "            outer_lr (float): learning rate for outer-loop optimization\n",
        "            log_dir (str): path to logging directory\n",
        "        \"\"\"\n",
        "        meta_parameters = {}\n",
        "\n",
        "        # construct feature extractor\n",
        "        in_channels = NUM_INPUT_CHANNELS\n",
        "        for i in range(NUM_CONV_LAYERS):\n",
        "            meta_parameters[f'conv{i}'] = nn.init.xavier_uniform_(\n",
        "                torch.empty(\n",
        "                    NUM_HIDDEN_CHANNELS,\n",
        "                    in_channels,\n",
        "                    KERNEL_SIZE,\n",
        "                    KERNEL_SIZE,\n",
        "                    requires_grad=True,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "            )\n",
        "            meta_parameters[f'b{i}'] = nn.init.zeros_(\n",
        "                torch.empty(\n",
        "                    NUM_HIDDEN_CHANNELS,\n",
        "                    requires_grad=True,\n",
        "                    device=DEVICE\n",
        "                )\n",
        "            )\n",
        "            in_channels = NUM_HIDDEN_CHANNELS\n",
        "\n",
        "        # construct linear head layer\n",
        "        meta_parameters[f'w{NUM_CONV_LAYERS}'] = nn.init.xavier_uniform_(\n",
        "            torch.empty(\n",
        "                num_outputs,\n",
        "                NUM_HIDDEN_CHANNELS,\n",
        "                requires_grad=True,\n",
        "                device=DEVICE\n",
        "            )\n",
        "        )\n",
        "        meta_parameters[f'b{NUM_CONV_LAYERS}'] = nn.init.zeros_(\n",
        "            torch.empty(\n",
        "                num_outputs,\n",
        "                requires_grad=True,\n",
        "                device=DEVICE\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self._meta_parameters = meta_parameters\n",
        "        self._num_inner_steps = num_inner_steps\n",
        "        self._inner_lrs = {\n",
        "            k: torch.tensor(inner_lr, requires_grad=learn_inner_lrs)\n",
        "            for k in self._meta_parameters.keys()\n",
        "        }\n",
        "        self._outer_lr = outer_lr\n",
        "\n",
        "        self._optimizer = torch.optim.Adam(\n",
        "            list(self._meta_parameters.values()) +\n",
        "            list(self._inner_lrs.values()),\n",
        "            lr=self._outer_lr\n",
        "        )\n",
        "        # self._log_dir = log_dir\n",
        "        # os.makedirs(self._log_dir, exist_ok=True)\n",
        "\n",
        "        self._start_train_step = 0\n",
        "\n",
        "    def _forward(self, images, parameters):\n",
        "        \"\"\"Computes predicted classification logits.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor): batch of Omniglot images\n",
        "                shape (num_images, channels, height, width)\n",
        "            parameters (dict[str, Tensor]): parameters to use for\n",
        "                the computation\n",
        "\n",
        "        Returns:\n",
        "            a Tensor consisting of a batch of logits\n",
        "                shape (num_images, classes)\n",
        "        \"\"\"\n",
        "        x = images\n",
        "        for i in range(NUM_CONV_LAYERS):\n",
        "            x = F.conv2d(\n",
        "                input=x,\n",
        "                weight=parameters[f'conv{i}'],\n",
        "                bias=parameters[f'b{i}'],\n",
        "                stride=1,\n",
        "                padding='same'\n",
        "            )\n",
        "            x = F.batch_norm(x, None, None, training=True)\n",
        "            x = F.relu(x)\n",
        "        x = torch.mean(x, dim=[2, 3])\n",
        "        return F.linear(\n",
        "            input=x,\n",
        "            weight=parameters[f'w{NUM_CONV_LAYERS}'],\n",
        "            bias=parameters[f'b{NUM_CONV_LAYERS}']\n",
        "        )\n",
        "\n",
        "    def _inner_loop(self, images, labels, train):\n",
        "        \"\"\"Computes the adapted network parameters via the MAML inner loop.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor): task support set inputs\n",
        "                shape (num_images, channels, height, width)\n",
        "            labels (Tensor): task support set outputs\n",
        "                shape (num_images,)\n",
        "            train (bool): whether we are training or evaluating (not necessary?)\n",
        "\n",
        "        Returns:\n",
        "            parameters (dict[str, Tensor]): adapted network parameters\n",
        "            accuracies (list[float]): support set accuracy over the course of\n",
        "                the inner loop, length num_inner_steps + 1\n",
        "        \"\"\"\n",
        "        accuracies = []\n",
        "        parameters = {\n",
        "            k: torch.clone(v)\n",
        "            for k, v in self._meta_parameters.items()\n",
        "        }\n",
        "        # This method computes the inner loop (adaptation) procedure for one\n",
        "        # task. It also scores the model along the way.\n",
        "        # Populate accuracies and update parameters.\n",
        "        # F.cross_entropy to compute classification losses.\n",
        "        # score() to compute accuracies.\n",
        "\n",
        "        # here we are doing \\phi_i = \\theta - inner_lr * grad(\\theta, L, D_{tr})\n",
        "        for _ in range(self._num_inner_steps):\n",
        "            logits = self._forward(images, parameters)\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "            # create graph due to computing second order derivatives in MAML\n",
        "            gradients = autograd.grad(\n",
        "                loss, parameters.values(), create_graph=True)\n",
        "\n",
        "            # update parameters\n",
        "            for i in range(len(parameters.keys())):\n",
        "                k = list(parameters.keys())[i]\n",
        "                v = list(parameters.values())[i]\n",
        "                assert v.shape == gradients[i].shape, 'Not proper shape'\n",
        "\n",
        "                parameters[k] = v - self._inner_lrs[k] * gradients[i]\n",
        "\n",
        "            acc = score(logits, labels)\n",
        "            accuracies.append(acc)\n",
        "\n",
        "        final_logits = self._forward(images, parameters)\n",
        "        final_acc = score(final_logits, labels)\n",
        "        accuracies.append(final_acc)\n",
        "        return parameters, accuracies\n",
        "\n",
        "    def _outer_step(self, task_batch, train):\n",
        "        \"\"\"Computes the MAML loss and metrics on a batch of tasks.\n",
        "\n",
        "        Args:\n",
        "            task_batch (tuple): batch of tasks from an Omniglot DataLoader\n",
        "            train (bool): whether we are training or evaluating\n",
        "\n",
        "        Returns:\n",
        "            outer_loss (Tensor): mean MAML loss over the batch, scalar\n",
        "            accuracies_support (ndarray): support set accuracy over the\n",
        "                course of the inner loop, averaged over the task batch\n",
        "                shape (num_inner_steps + 1,)\n",
        "            accuracy_query (float): query set accuracy of the adapted\n",
        "                parameters, averaged over the task batch\n",
        "        \"\"\"\n",
        "        outer_loss_batch = []\n",
        "        accuracies_support_batch = []\n",
        "        accuracy_query_batch = []\n",
        "        for task in task_batch:\n",
        "            images_support, labels_support, images_query, labels_query = task\n",
        "            images_support = images_support.to(DEVICE)\n",
        "            labels_support = labels_support.to(DEVICE)\n",
        "            images_query = images_query.to(DEVICE)\n",
        "            labels_query = labels_query.to(DEVICE)\n",
        "            # For a given task, use the _inner_loop method to adapt, then\n",
        "            # compute the MAML loss and other metrics.\n",
        "            # F.cross_entropy to compute classification losses.\n",
        "            # score() to compute accuracies.\n",
        "            # Populate outer_loss_batch, accuracies_support_batch,\n",
        "            # and accuracy_query_batch.\n",
        "\n",
        "            # computes \\phi_L\n",
        "            parameters, supp_accs = self._inner_loop(\n",
        "                images_support, labels_support, train)\n",
        "\n",
        "            # gets the loss w.r.t. \\phi_L\n",
        "            logits = self._forward(images_query, parameters)\n",
        "            loss = F.cross_entropy(logits, labels_query)\n",
        "            outer_loss_batch.append(loss)\n",
        "\n",
        "            accuracies_support_batch.append(supp_accs)\n",
        "            q_accs = score(logits, labels_query)\n",
        "            accuracy_query_batch.append(q_accs)\n",
        "\n",
        "        outer_loss = torch.mean(torch.stack(outer_loss_batch))\n",
        "        accuracies_support = np.mean(\n",
        "            accuracies_support_batch,\n",
        "            axis=0\n",
        "        )\n",
        "        accuracy_query = np.mean(accuracy_query_batch)\n",
        "        return outer_loss, accuracies_support, accuracy_query"
      ],
      "metadata": {
        "id": "aeQ39GcTQTAC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}