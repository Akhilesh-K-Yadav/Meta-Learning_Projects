{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNhWLgChpP9QsSDc1vrpzW7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilesh-K-Yadav/Meta-Learning_Projects/blob/main/Prototypical_Net/ProtoNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NEEVnkBZzpjB"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "import google_drive_downloader as gdd\n",
        "import imageio\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.utils.data import dataset, sampler, dataloader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import autograd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils import tensorboard\n",
        "#import pdb\n",
        "\n",
        "NUM_TRAIN_CLASSES = 1100\n",
        "NUM_VAL_CLASSES = 100\n",
        "NUM_TEST_CLASSES = 423\n",
        "NUM_SAMPLES_PER_CLASS = 20\n",
        "\n",
        "NUM_INPUT_CHANNELS = 1\n",
        "NUM_HIDDEN_CHANNELS = 64\n",
        "KERNEL_SIZE = 3\n",
        "NUM_CONV_LAYERS = 4\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "SUMMARY_INTERVAL = 10\n",
        "SAVE_INTERVAL = 100\n",
        "PRINT_INTERVAL = 10\n",
        "VAL_INTERVAL = PRINT_INTERVAL * 5\n",
        "NUM_TEST_TASKS = 600\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_PATH = './omniglot_resized'\n",
        "GDD_FILE_ID = '1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI'\n",
        "gdd.GoogleDriveDownloader.download_file_from_google_drive(\n",
        "                file_id=GDD_FILE_ID,\n",
        "                dest_path=f'{BASE_PATH}.zip',\n",
        "                unzip=True\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvykCZKm0Hui",
        "outputId": "5361e983-3e44-45ba-9f1b-6c42b3eb2262"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading 1iaSFXIYC3AB8q9K_M-oVMa4pmB7yKMtI into ./omniglot_resized.zip... Done.\n",
            "Unzipping...Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(file_path):\n",
        "    \"\"\"Loads and transforms an Omniglot image.\n",
        "    Args:\n",
        "        file_path (str): file path of image\n",
        "    Returns:\n",
        "        a Tensor containing image data\n",
        "            shape (1, 28, 28)\n",
        "    \"\"\"\n",
        "    x = imageio.imread(file_path)\n",
        "    x = torch.tensor(x, dtype=torch.float32).reshape([1, 28, 28])\n",
        "    x = x / 255.0\n",
        "    return 1 - x"
      ],
      "metadata": {
        "id": "gUF4xomA0I4v"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotDataset(dataset.Dataset):\n",
        "    \"\"\"Omniglot dataset for meta-learning.\n",
        "\n",
        "    Each element of the dataset is a task. A task is specified with a key,\n",
        "    which is a tuple of class indices (no particular order). The corresponding\n",
        "    value is the instantiated task, which consists of sampled (image, label)\n",
        "    pairs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_support, num_query):\n",
        "        \"\"\"Inits OmniglotDataset.\n",
        "\n",
        "        Args:\n",
        "            num_support (int): number of support examples per class\n",
        "            num_query (int): number of query examples per class\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # get all character folders\n",
        "        self._character_folders = glob.glob(\n",
        "            os.path.join(BASE_PATH, '*/*/'))\n",
        "        assert len(self._character_folders) == (\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
        "        )\n",
        "\n",
        "        # shuffle characters\n",
        "        np.random.default_rng(0).shuffle(self._character_folders)\n",
        "\n",
        "        # check problem arguments\n",
        "        assert num_support + num_query <= NUM_SAMPLES_PER_CLASS\n",
        "        self._num_support = num_support\n",
        "        self._num_query = num_query\n",
        "\n",
        "    def __getitem__(self, class_idxs):\n",
        "        \"\"\"Constructs a task.\n",
        "\n",
        "        Data for each class is sampled uniformly at random without replacement.\n",
        "        The ordering of the labels corresponds to that of class_idxs.\n",
        "\n",
        "        Args:\n",
        "            class_idxs (tuple[int]): class indices that comprise the task\n",
        "\n",
        "        Returns:\n",
        "            images_support (Tensor): task support images\n",
        "                shape (num_way * num_support, channels, height, width)\n",
        "            labels_support (Tensor): task support labels\n",
        "                shape (num_way * num_support,)\n",
        "            images_query (Tensor): task query images\n",
        "                shape (num_way * num_query, channels, height, width)\n",
        "            labels_query (Tensor): task query labels\n",
        "                shape (num_way * num_query,)\n",
        "        \"\"\"\n",
        "        images_support, images_query = [], []\n",
        "        labels_support, labels_query = [], []\n",
        "\n",
        "        for label, class_idx in enumerate(class_idxs):\n",
        "            # get a class's examples and sample from them\n",
        "            all_file_paths = glob.glob(\n",
        "                os.path.join(self._character_folders[class_idx], '*.png')\n",
        "            )\n",
        "            sampled_file_paths = np.random.default_rng().choice(\n",
        "                all_file_paths,\n",
        "                size=self._num_support + self._num_query,\n",
        "                replace=False\n",
        "            )\n",
        "            images = [load_image(file_path) for file_path in sampled_file_paths]\n",
        "\n",
        "            # split sampled examples into support and query\n",
        "            images_support.extend(images[:self._num_support])\n",
        "            images_query.extend(images[self._num_support:])\n",
        "            labels_support.extend([label] * self._num_support)\n",
        "            labels_query.extend([label] * self._num_query)\n",
        "\n",
        "        # aggregate into tensors\n",
        "        images_support = torch.stack(images_support)  # shape (N*S, C, H, W)\n",
        "        labels_support = torch.tensor(labels_support)  # shape (N*S)\n",
        "        images_query = torch.stack(images_query)\n",
        "        labels_query = torch.tensor(labels_query)\n",
        "\n",
        "        return images_support, labels_support, images_query, labels_query\n"
      ],
      "metadata": {
        "id": "ALxfqljP0NaS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OmniglotSampler(sampler.Sampler):\n",
        "    \"\"\"Samples task specification keys for an OmniglotDataset.\"\"\"\n",
        "\n",
        "    def __init__(self, split_idxs, num_way, num_tasks):\n",
        "\n",
        "      \"\"\"Inits OmniglotSampler.\n",
        "\n",
        "      Args:\n",
        "          split_idxs (range): indices that comprise the\n",
        "              training/validation/test split\n",
        "          num_way (int): number of classes per task\n",
        "          num_tasks (int): number of tasks to sample\n",
        "      \"\"\"\n",
        "      super().__init__(None)\n",
        "\n",
        "      self.split_idxs = split_idxs\n",
        "      self.num_way = num_way\n",
        "      self.num_tasks = num_tasks\n",
        "\n",
        "    def __iter__(self):\n",
        "      return (\n",
        "          np.random.default_rng().choice(\n",
        "              self.split_idxs,\n",
        "              size=self.num_way,\n",
        "              replace=False\n",
        "          ) for _ in range(self.num_tasks)\n",
        "      )\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_tasks"
      ],
      "metadata": {
        "id": "dZp1Ws3c0Vnv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identity(x):\n",
        "    return x\n",
        "\n",
        "def get_omniglot_dataloader(\n",
        "        split,\n",
        "        batch_size,\n",
        "        num_way,\n",
        "        num_support,\n",
        "        num_query,\n",
        "        num_tasks_per_epoch\n",
        "):\n",
        "    \"\"\"Returns a dataloader.DataLoader for Omniglot.\n",
        "\n",
        "    Args:\n",
        "        split (str): one of 'train', 'val', 'test'\n",
        "        batch_size (int): number of tasks per batch\n",
        "        num_way (int): number of classes per task\n",
        "        num_support (int): number of support examples per class\n",
        "        num_query (int): number of query examples per class\n",
        "        num_tasks_per_epoch (int): number of tasks before DataLoader is\n",
        "            exhausted\n",
        "    \"\"\"\n",
        "\n",
        "    if split == 'train':\n",
        "        split_idxs = range(NUM_TRAIN_CLASSES)\n",
        "    elif split == 'val':\n",
        "        split_idxs = range(\n",
        "            NUM_TRAIN_CLASSES,\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES\n",
        "        )\n",
        "    elif split == 'test':\n",
        "        split_idxs = range(\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES,\n",
        "            NUM_TRAIN_CLASSES + NUM_VAL_CLASSES + NUM_TEST_CLASSES\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    return dataloader.DataLoader(\n",
        "        dataset=OmniglotDataset(num_support, num_query),\n",
        "        batch_size=batch_size,\n",
        "        sampler=OmniglotSampler(split_idxs, num_way, num_tasks_per_epoch),\n",
        "        num_workers=2,\n",
        "        collate_fn=identity,\n",
        "        pin_memory=torch.cuda.is_available(),\n",
        "        drop_last=True\n",
        "    )"
      ],
      "metadata": {
        "id": "U7IolBbu0b4g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image = imageio.imread(\"/content/omniglot_resized/Alphabet_of_the_Magi/character01/0709_01.png\")\n",
        "image.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2RzoS60n3C",
        "outputId": "52060d6f-28ab-4f7f-bbbd-ed0744ed324a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-913cf8e1afe4>:1: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(\"/content/omniglot_resized/Alphabet_of_the_Magi/character01/0709_01.png\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "samples = get_omniglot_dataloader(\n",
        "        split='train',\n",
        "        batch_size=1,\n",
        "        num_way=4,\n",
        "        num_support=1,\n",
        "        num_query=1,\n",
        "        num_tasks_per_epoch=1\n",
        ")\n",
        "tasks = next(iter(samples))\n",
        "for task in tasks:\n",
        "  images_support, labels_support, images_query, labels_query = task\n",
        "images_support.shape, labels_support.shape, images_query.shape, labels_query.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqhq6xkQ0qre",
        "outputId": "855ffff3-bb61-45d5-ec72-8061472b9937"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-a6327f0e4a5e>:9: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  x = imageio.imread(file_path)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([4, 1, 28, 28]),\n",
              " torch.Size([4]),\n",
              " torch.Size([4, 1, 28, 28]),\n",
              " torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(images_support[0][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "O65bNVn10v0G",
        "outputId": "09a44ba7-5778-465c-9f80-64c9e31b6f51"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7b93daff2170>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels_support"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "20N0o_wg02Yy",
        "outputId": "1aa45188-3797-422e-8f10-fb8084d7c41c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def score(logits, labels):\n",
        "    \"\"\"Returns the mean accuracy of a model's predictions on a set of examples.\n",
        "\n",
        "    Args:\n",
        "        logits (torch.Tensor): model predicted logits\n",
        "            shape (examples, classes)\n",
        "        labels (torch.Tensor): classification labels from 0 to num_classes - 1\n",
        "            shape (examples,)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    assert logits.dim() == 2\n",
        "    assert labels.dim() == 1\n",
        "    assert logits.shape[0] == labels.shape[0]\n",
        "    y = torch.argmax(logits, dim=-1) == labels\n",
        "    y = y.type(torch.float)\n",
        "    return torch.mean(y).item()"
      ],
      "metadata": {
        "id": "Z_wKatLO08LY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProtoNetNetwork(nn.Module):\n",
        "    \"\"\"Container for ProtoNet weights and image-to-latent computation.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Inits ProtoNetNetwork.\n",
        "\n",
        "        The network consists of four convolutional blocks, each comprising a\n",
        "        convolution layer, a batch normalization layer, ReLU activation, and 2x2\n",
        "        max pooling for downsampling. There is an additional flattening\n",
        "        operation at the end.\n",
        "\n",
        "        Note that unlike conventional use, batch normalization is always done\n",
        "        with batch statistics, regardless of whether we are training or\n",
        "        evaluating. This technically makes meta-learning transductive, as\n",
        "        opposed to inductive.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        in_channels = NUM_INPUT_CHANNELS\n",
        "        for _ in range(NUM_CONV_LAYERS):\n",
        "            layers.append(\n",
        "                nn.Conv2d(\n",
        "                    in_channels,\n",
        "                    NUM_HIDDEN_CHANNELS,\n",
        "                    (KERNEL_SIZE, KERNEL_SIZE),\n",
        "                    padding='same'\n",
        "                )\n",
        "            )\n",
        "            layers.append(nn.BatchNorm2d(NUM_HIDDEN_CHANNELS))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.MaxPool2d(2))\n",
        "            in_channels = NUM_HIDDEN_CHANNELS\n",
        "        layers.append(nn.Flatten())\n",
        "        self._layers = nn.Sequential(*layers)\n",
        "        self.to(DEVICE)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"Computes the latent representation of a batch of images.\n",
        "\n",
        "        Args:\n",
        "            images (Tensor): batch of Omniglot images\n",
        "                shape (num_images, channels, height, width)\n",
        "\n",
        "        Returns:\n",
        "            a Tensor containing a batch of latent representations\n",
        "                shape (num_images, latents)\n",
        "        \"\"\"\n",
        "        return self._layers(images)\n"
      ],
      "metadata": {
        "id": "ckVpocCD1CFL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProtoNet:\n",
        "    \"\"\"Trains and assesses a prototypical network.\"\"\"\n",
        "\n",
        "    def __init__(self, learning_rate, log_dir):\n",
        "        \"\"\"Inits ProtoNet.\n",
        "\n",
        "        Args:\n",
        "            learning_rate (float): learning rate for the Adam optimizer\n",
        "            log_dir (str): path to logging directory\n",
        "        \"\"\"\n",
        "\n",
        "        self._network = ProtoNetNetwork()\n",
        "        self._optimizer = torch.optim.Adam(\n",
        "            self._network.parameters(),\n",
        "            lr=learning_rate\n",
        "        )\n",
        "        self._log_dir = log_dir\n",
        "        os.makedirs(self._log_dir, exist_ok=True)\n",
        "\n",
        "        self._start_train_step = 0\n",
        "\n",
        "    def _step(self, task_batch):\n",
        "        \"\"\"Computes ProtoNet mean loss (and accuracy) on a batch of tasks.\n",
        "\n",
        "        Args:\n",
        "            task_batch (tuple[Tensor, Tensor, Tensor, Tensor]):\n",
        "                batch of tasks from an Omniglot DataLoader\n",
        "\n",
        "        Returns:\n",
        "            a Tensor containing mean ProtoNet loss over the batch\n",
        "                shape ()\n",
        "            mean support set accuracy over the batch as a float\n",
        "            mean query set accuracy over the batch as a float\n",
        "        \"\"\"\n",
        "\n",
        "        loss_batch = []\n",
        "        accuracy_support_batch = []\n",
        "        accuracy_query_batch = []\n",
        "        for task in task_batch:\n",
        "            images_support, labels_support, images_query, labels_query = task\n",
        "            images_support = images_support.to(DEVICE)\n",
        "            labels_support = labels_support.to(DEVICE)\n",
        "            images_query = images_query.to(DEVICE)\n",
        "            labels_query = labels_query.to(DEVICE)\n",
        "            # For a given task, compute the prototypes and the protonet loss.\n",
        "            # F.cross_entropy to compute classification losses.\n",
        "            # score to compute accuracies.\n",
        "            # Populate loss_batch, accuracy_support_batch, and\n",
        "            # accuracy_query_batch.\n",
        "\n",
        "            # we need to compute all of the prototypes for all classes in the task training batch\n",
        "            supp_feats = self._network(images_support)\n",
        "\n",
        "            # get all classes, and then score\n",
        "            classes = torch.unique(labels_support, sorted=True)\n",
        "            means = []\n",
        "            for c in classes:\n",
        "                idxes = (labels_support == c).nonzero()\n",
        "                class_feats = supp_feats[idxes]\n",
        "                prototype = torch.mean(class_feats, dim=0)\n",
        "                means.append(prototype)\n",
        "\n",
        "            means = torch.cat(means)  # (num_classes, feature_dim)\n",
        "\n",
        "            # now to measure distances to all means\n",
        "            # (batch_size, feature_dim)\n",
        "            query_feats = self._network(images_query)\n",
        "            # (batch_size, num_classes, feature_dim)\n",
        "            query_feats = torch.stack([query_feats] * means.size(0), dim=1)\n",
        "\n",
        "            exp_means = means.expand(*query_feats.shape)\n",
        "\n",
        "            # (batch_size, num_classes, feature_dim)\n",
        "            query_diffs = query_feats - exp_means\n",
        "            # (batch_size, num_classes)\n",
        "            query_sq_norms = torch.norm(query_diffs, p=2, dim=-1).square()\n",
        "            query_logits = -query_sq_norms\n",
        "            # query_norms[i][j] = d(f_\\theta(x_i), c_j)\n",
        "\n",
        "            loss = F.cross_entropy(query_logits, labels_query)\n",
        "            loss_batch.append(loss)\n",
        "\n",
        "            # compute accuracies\n",
        "            query_acc =  score(query_logits, labels_query)\n",
        "            accuracy_query_batch.append(query_acc)\n",
        "\n",
        "            # (batch_size, num_classes, feature_dim)\n",
        "            supp_feats = torch.stack([supp_feats] * means.size(0), dim=1)\n",
        "\n",
        "            exp_means = means.expand(*supp_feats.shape)\n",
        "\n",
        "            # (batch_size, num_classes, feature_dim)\n",
        "            supp_diffs = supp_feats - exp_means\n",
        "            # (batch_size, num_classes)\n",
        "            supp_sq_norms = torch.norm(supp_diffs, p=2, dim=2).square()\n",
        "            supp_logits = -supp_sq_norms\n",
        "            supp_acc =  score(supp_logits, labels_support)\n",
        "            accuracy_support_batch.append(supp_acc)\n",
        "\n",
        "\n",
        "        return (\n",
        "            torch.mean(torch.stack(loss_batch)),\n",
        "            np.mean(accuracy_support_batch),\n",
        "            np.mean(accuracy_query_batch)\n",
        "        )\n",
        "\n",
        "    def train(self, dataloader_train, dataloader_val, writer):\n",
        "        \"\"\"Train the ProtoNet.\n",
        "\n",
        "        Consumes dataloader_train to optimize weights of ProtoNetNetwork\n",
        "        while periodically validating on dataloader_val, logging metrics, and\n",
        "        saving checkpoints.\n",
        "\n",
        "        Args:\n",
        "            dataloader_train (DataLoader): loader for train tasks\n",
        "            dataloader_val (DataLoader): loader for validation tasks\n",
        "            writer (SummaryWriter): TensorBoard logger\n",
        "        \"\"\"\n",
        "        print(f'Starting training at iteration {self._start_train_step}.')\n",
        "        for i_step, task_batch in enumerate(\n",
        "                dataloader_train,\n",
        "                start=self._start_train_step\n",
        "        ):\n",
        "            self._optimizer.zero_grad()\n",
        "            loss, accuracy_support, accuracy_query = self._step(task_batch)\n",
        "            loss.backward()\n",
        "            self._optimizer.step()\n",
        "\n",
        "            if i_step % PRINT_INTERVAL == 0:\n",
        "                print(\n",
        "                    f'Iteration {i_step}: '\n",
        "                    f'loss: {loss.item():.3f}, '\n",
        "                    f'support accuracy: {accuracy_support.item():.3f}, '\n",
        "                    f'query accuracy: {accuracy_query.item():.3f}'\n",
        "                )\n",
        "                writer.add_scalar('loss/train', loss.item(), i_step)\n",
        "                writer.add_scalar(\n",
        "                    'train_accuracy/support',\n",
        "                    accuracy_support.item(),\n",
        "                    i_step\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    'train_accuracy/query',\n",
        "                    accuracy_query.item(),\n",
        "                    i_step\n",
        "                )\n",
        "\n",
        "            if i_step % VAL_INTERVAL == 0:\n",
        "                with torch.no_grad():\n",
        "                    losses, accuracies_support, accuracies_query = [], [], []\n",
        "                    for val_task_batch in dataloader_val:\n",
        "                        loss, accuracy_support, accuracy_query = (\n",
        "                            self._step(val_task_batch)\n",
        "                        )\n",
        "                        losses.append(loss.item())\n",
        "                        accuracies_support.append(accuracy_support)\n",
        "                        accuracies_query.append(accuracy_query)\n",
        "                    loss = np.mean(losses)\n",
        "                    accuracy_support = np.mean(accuracies_support)\n",
        "                    accuracy_query = np.mean(accuracies_query)\n",
        "                print(\n",
        "                    f'Validation: '\n",
        "                    f'loss: {loss:.3f}, '\n",
        "                    f'support accuracy: {accuracy_support:.3f}, '\n",
        "                    f'query accuracy: {accuracy_query:.3f}'\n",
        "                )\n",
        "                writer.add_scalar('loss/val', loss, i_step)\n",
        "                writer.add_scalar(\n",
        "                    'val_accuracy/support',\n",
        "                    accuracy_support,\n",
        "                    i_step\n",
        "                )\n",
        "                writer.add_scalar(\n",
        "                    'val_accuracy/query',\n",
        "                    accuracy_query,\n",
        "                    i_step\n",
        "                )\n",
        "\n",
        "            if i_step % SAVE_INTERVAL == 0:\n",
        "                self._save(i_step)\n",
        "\n",
        "    def test(self, dataloader_test):\n",
        "        \"\"\"Evaluate the ProtoNet on test tasks.\n",
        "\n",
        "        Args:\n",
        "            dataloader_test (DataLoader): loader for test tasks\n",
        "        \"\"\"\n",
        "        accuracies = []\n",
        "        for task_batch in dataloader_test:\n",
        "            accuracies.append(self._step(task_batch)[2])\n",
        "        mean = np.mean(accuracies)\n",
        "        std = np.std(accuracies)\n",
        "        mean_95_confidence_interval = 1.96 * std / np.sqrt(NUM_TEST_TASKS)\n",
        "        print(\n",
        "            f'Accuracy over {NUM_TEST_TASKS} test tasks: '\n",
        "            f'mean {mean:.3f}, '\n",
        "            f'95% confidence interval {mean_95_confidence_interval:.3f}'\n",
        "        )\n",
        "\n",
        "    def load(self, checkpoint_step):\n",
        "        \"\"\"Loads a checkpoint.\n",
        "\n",
        "        Args:\n",
        "            checkpoint_step (int): iteration of checkpoint to load\n",
        "\n",
        "        Raises:\n",
        "            ValueError: if checkpoint for checkpoint_step is not found\n",
        "        \"\"\"\n",
        "        target_path = (\n",
        "            f'{os.path.join(self._log_dir, \"state\")}'\n",
        "            f'{checkpoint_step}.pt'\n",
        "        )\n",
        "        if os.path.isfile(target_path):\n",
        "            state = torch.load(target_path)\n",
        "            self._network.load_state_dict(state['network_state_dict'])\n",
        "            self._optimizer.load_state_dict(state['optimizer_state_dict'])\n",
        "            self._start_train_step = checkpoint_step + 1\n",
        "            print(f'Loaded checkpoint iteration {checkpoint_step}.')\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f'No checkpoint for iteration {checkpoint_step} found.'\n",
        "            )\n",
        "\n",
        "    def _save(self, checkpoint_step):\n",
        "        \"\"\"Saves network and optimizer state_dicts as a checkpoint.\n",
        "\n",
        "        Args:\n",
        "            checkpoint_step (int): iteration to label checkpoint with\n",
        "        \"\"\"\n",
        "        torch.save(\n",
        "            dict(network_state_dict=self._network.state_dict(),\n",
        "                 optimizer_state_dict=self._optimizer.state_dict()),\n",
        "            f'{os.path.join(self._log_dir, \"state\")}{checkpoint_step}.pt'\n",
        "        )\n",
        "        print('Saved checkpoint.')\n"
      ],
      "metadata": {
        "id": "LPxb-b1g3Cf2"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    log_dir = None\n",
        "    num_way = 5\n",
        "    num_support = 1\n",
        "    num_query = 5\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 5\n",
        "    num_train_iterations = 10000\n",
        "    test = False\n",
        "    checkpoint_step = -1\n",
        "\n",
        "    if log_dir is None:\n",
        "        log_dir = f'./logs/protonet/omniglot.way:{ num_way}.support:{ num_support}.query:{ num_query}.lr:{ learning_rate}.batch_size:{ batch_size}'\n",
        "    print(f'log_dir: {log_dir}')\n",
        "    writer = tensorboard.SummaryWriter(log_dir=log_dir)\n",
        "\n",
        "    protonet = ProtoNet( learning_rate, log_dir)\n",
        "\n",
        "    if  checkpoint_step > -1:\n",
        "        protonet.load( checkpoint_step)\n",
        "    else:\n",
        "        print('Checkpoint loading skipped.')\n",
        "\n",
        "    if not  test:\n",
        "        num_training_tasks =  batch_size * ( num_train_iterations -\n",
        "                                                 checkpoint_step - 1)\n",
        "        print(\n",
        "            f'Training on tasks with composition '\n",
        "            f'num_way={ num_way}, '\n",
        "            f'num_support={ num_support}, '\n",
        "            f'num_query={ num_query}'\n",
        "        )\n",
        "        dataloader_train = get_omniglot_dataloader(\n",
        "            'train',\n",
        "            batch_size,\n",
        "            num_way,\n",
        "            num_support,\n",
        "            num_query,\n",
        "            num_training_tasks\n",
        "        )\n",
        "        dataloader_val = get_omniglot_dataloader(\n",
        "            'val',\n",
        "            batch_size,\n",
        "            num_way,\n",
        "            num_support,\n",
        "            num_query,\n",
        "            batch_size * 4\n",
        "        )\n",
        "        protonet.train(\n",
        "            dataloader_train,\n",
        "            dataloader_val,\n",
        "            writer\n",
        "        )\n",
        "    else:\n",
        "        print(\n",
        "            f'Testing on tasks with composition '\n",
        "            f'num_way={ num_way}, '\n",
        "            f'num_support={num_support}, '\n",
        "            f'num_query={num_query}'\n",
        "        )\n",
        "        dataloader_test = get_omniglot_dataloader(\n",
        "            'test',\n",
        "            1,\n",
        "            num_way,\n",
        "            num_support,\n",
        "            num_query,\n",
        "            NUM_TEST_TASKS\n",
        "        )\n",
        "        protonet.test(dataloader_test)"
      ],
      "metadata": {
        "id": "zt1gqocf3Ny-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "JoU5v2Wn3Uye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6hMsUOlNwesQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}